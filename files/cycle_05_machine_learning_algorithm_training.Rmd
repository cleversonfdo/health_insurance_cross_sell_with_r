---
title: "Machine Learning Algorithms Training - Cycle 05"
author: "Cleverson Oliveira"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output:
  html_document:
    highlight: textmate
    logo: logo.png
    theme: jou
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
    df_print: paged
    code_folding: hide
  pdf_document:
    toc: yes
---

```{r, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Imports

```{r, echo=FALSE}
#install.packages("devtools")
#devtools::install_github("ropensci/skimr")
#install.packages("gtsummary")
#install.packages("magick")
#install.packages("summarytools")
#install.packages("knitr")
#install.packages("kableExtra")
#devtools::install_github("kupietz/kableExtra")
#install.packages("gridExtra")
#install.packages("reshape2")
#install.packages("tidymodels")
#install.packages("randomForest")
#install.packages("tidyverse")
#install.packages("RcppEigen")
#install.packages("ranger")
#install.packages("xgboost")
#install.packages("kknn")
```

```{r}
library(dplyr)
library(tidyverse)
library(janitor)
library(gtsummary)
library(summarytools)
library(knitr)
library(kableExtra)
library(gridExtra)
library(readr)
library(ggplot2)
library(tidymodels)
library(randomForest)
library(reshape2)
```

# Helper Functions

```{r functions}
# Top @K precision and recall function ---------
metrics_at_k_function <- function(model_results, k){
  df_results <- model_results %>% 
                arrange(desc(.pred_yes)) %>% 
                mutate(
                  TP = ifelse(.pred_class == "yes" & response == "yes", 1, 0),
                  FP = ifelse(.pred_class == "yes" & response == "no", 1, 0),
                  FN = ifelse(.pred_class == "no" & response == "yes", 1, 0),
                  TN = ifelse(.pred_class == "no" & response == "no", 1, 0)
                ) #%>% View()
  
  # Create list for precision and recall
  precision_at_k <- list()
  recall_at_k <- list()

  # Populate the metric list
  for (i in 1:k) {
    subset_k <- df_results %>% 
                dplyr_row_slice(1:i)
    
    precision_at_k[[i]] <- (subset_k$TP %>% sum())/(subset_k$TP %>% sum() + subset_k$FP %>% sum())
    recall_at_k[[i]] <- (subset_k$TP %>% sum())/(subset_k$TP %>% sum() + subset_k$FN %>% sum())
  }

  metrics_at_k <- df_results %>% 
                  dplyr_row_slice(1:k) %>% 
                  mutate(
                    precision_at_k = unlist(precision_at_k),
                    recall_at_k = unlist(recall_at_k)
                  )
  metrics_at_k %>% View()
  return(metrics_at_k)
}

# Final metrics @K Function ----------------------
final_metrics_at_k <- function(model_results, k){
  
  model_metrics_at_k <- metrics_at_k_function(model_results, k)

  model_metrics_at_k %>% 
    dplyr::slice(k) %>% 
    select(precision_at_k, recall_at_k)
  
  # model_metrics_at_k
}
```

# Data Collection

```{r collection}
df <- readRDS("df4.rds")

selected_columns <- c(
  "id", 
  "age",
  "vehicle_damage",
  "days_associated",
  "previously_insured",
  "health_annual_paid", 
  "policy_sales_channel", 
  "region_code",
  "response"
)

# Final dataset
df5 <- df %>% select(all_of(selected_columns)) 

View(df5)
glimpse(df5)

saveRDS(df5, "df5.rds")
```

# Column Description

```{r}
variables <- df5 %>% names()
description <- c(
  "Unique ID for the customer",
  "Age of the customer",
  "Customer got his/her vehicle damaged in the past (yes/no)",
  "Number of Days, Customer has been associated with the company",
  "Customer already has Vehicle Insurance (yes/no)",
  "The amount customer needs to pay as premium in the year",
  "Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.",
  "Unique code for the region of the customer",
  "Customer is interested in car insurance (yes/no)"
)

df_description <- tibble(variables = variables,
       description = description)

kable(df_description, format = "html") %>% 
  kableExtra::kable_styling(bootstrap_options = "striped",
                            full_width = FALSE)
```

# Pre-processing

```{r}
# gender_encoder won't be necessay anymore because the gender column isn't in the selected columns list

region_encoder <- readRDS("region_encoder.rds")
policy_encoder <- readRDS("policy_encoder.rds")

# Create function
encoder_function <- function(df){
  df %>% 
  left_join(region_encoder) %>% 
  select(-region_code) %>% 
  rename(region_code = region_num) %>% 
  left_join(policy_encoder) %>% 
  select(-policy_sales_channel) %>% 
  rename(policy_sales_channel = policy_num) 
}
```

```{r}
df5 <- encoder_function(df5)
```

## Split into train and test datasets

```{r}
set.seed(123)

df_split <- df5 %>% 
            initial_split(prop = 0.80, strata = response)

df_train <- df_split %>% 
            training()

df_test <- df_split %>% 
           testing()

# Taking a look on the datasets
df_train
df_test
```

## Applying steps

```{r}
                    # explain response based on all variable, except id
df_recipe <- recipe(response ~ ., data = df_train %>% select(-id)) %>% 
             step_normalize(age, days_associated) %>% 
             step_scale(health_annual_paid) %>% # lot of outliers
             step_dummy(all_nominal(), -all_outcomes()) # outcomes = response variable
```

```{r}
# Train the recipe
df_prep <- df_recipe %>% 
           prep(training = df_train)

df_train_preprocessed <- df_prep %>% 
                         bake(new_data = df_train)

df_test_preprocessed <- df_prep %>% 
                        bake(new_data = df_test)
```

# Logistic Regression

```{r}
# Model Specification -----------
logistic_model <- logistic_reg() %>% 
                  set_engine('glm') %>% 
                  set_mode('classification')
```

```{r}
# Model Fitting -----------
start_time <- Sys.time()

logistic_fit <- logistic_model %>% 
                fit(response ~., data = df_train_preprocessed)

end_time <- Sys.time()

print(end_time - start_time)

saveRDS(logistic_fit, "logistic_fit.rds")
```

```{r}
logistic_fit <- readRDS("logistic_fit.rds")

# Prediction ----------
class_preds <- logistic_fit %>% 
               predict(new_data = df_test_preprocessed, type = 'class')

prob_preds <- logistic_fit %>% 
              predict(new_data = df_test_preprocessed, type = 'prob')

# Combine results -----------
lr_results <- df_test %>%  # probability of yes and no
              select(id, response) %>% 
              bind_cols(class_preds, prob_preds)

# Confusion Matrix ------------
confusion_matrix_lr <-  conf_mat(lr_results, truth = response, estimate = .pred_class)
```

```{r}
# Calculating metrics @K
lr_final_at_k_metrics <- tibble(model = "Logistic Regression") %>% 
                         bind_cols(final_metrics_at_k(lr_results, 2000))

lr_final_at_k_metrics
```

## Gain & Lift Curves

```{r}
# Gain curve
gain_curve(lr_results, response, .pred_yes) %>% 
  autoplot()
```

```{r}
# Lift curve
lift_curve(lr_results, response, .pred_yes) %>% 
  autoplot()
```

Gain: By approaching 25% of the ordered list , \~ 60% of all interested clients are reached.

Lift: By approaching 25% of the ordered list , the model performed \~2.3 times better than a random list.

# Decision Tree ðŸ’»

Time to fit the model: 1.065924 secs.

```{r}
# Model Specification -----------
dt_model <- decision_tree(tree_depth = 10) %>% 
            set_engine('rpart') %>% 
            set_mode('classification')

# Model Fitting -----------
start_time <- Sys.time()

dt_fit <- dt_model %>% 
          fit(response ~., data = df_train_preprocessed)

end_time <- Sys.time()

print(end_time - start_time)

saveRDS(dt_model, "dt_model.rds")
```

```{r}
dt_model <- readRDS("dt_model.rds")

# Prediction ----------
class_preds <- dt_fit %>% 
               predict(new_data = df_test_preprocessed, type = 'class')

prob_preds <- dt_fit %>% 
              predict(new_data = df_test_preprocessed, type = 'prob')

# Combine results -----------
dt_results <- df_test %>% 
              select(id, response) %>% 
              bind_cols(class_preds, prob_preds)

dt_results

# Confusion Matrix ------------
confusion_matrix_dt <-  conf_mat(dt_results, truth = response, estimate = .pred_class)
```

```{r}
# Calculating metrics @K
dt_final_at_k_metrics <- tibble(model = "Decision tree") %>% 
                         bind_cols(final_metrics_at_k(dt_results, 2000))

dt_final_at_k_metrics
```

```{r}
# Gain and Lift Curves
gain_curve(dt_results, response, .pred_yes) %>% 
  autoplot()

lift_curve(dt_results, response, .pred_yes) %>% 
  autoplot()
```

```{r}
dt_results %>% 
  select(.pred_yes, .pred_no) %>% 
  summary()
```

As the probabilities are constant throughout the rows there is no way to calculate gain and lift. So, this is not a good model to rank our clients as it isn't better than a random list.

# Random Forest ðŸ’»

Time to run the model: 8.450113 mins

```{r}
# Model Specification -----------
rf_model <- rand_forest(mtry = 3, 
                        trees = 1000, 
                        min_n = 100) %>% 
            set_engine('ranger') %>% 
            set_mode('classification')

# Model Fitting -----------
start_time <- Sys.time()

rf_fit <- rf_model %>% 
          fit(response ~., data = df_train_preprocessed)

end_time <- Sys.time()

print(end_time - start_time)

saveRDS(rf_model, "rf_model.rds")
```

```{r}
rf_model <- readRDS("rf_model.rds")

# Prediction ----------
class_preds <- rf_fit %>% 
               predict(new_data = df_test_preprocessed, type = 'class')

prob_preds <- rf_fit %>% 
              predict(new_data = df_test_preprocessed, type = 'prob')

# Combine results -----------
rf_results <- df_test %>% 
              select(id, response) %>% 
              bind_cols(class_preds, prob_preds)

# Confusion Matrix ------------
confusion_matrix_rf <- conf_mat(rf_results, truth = response, estimate = .pred_class)
```

```{r}
# Calculating metrics @K
rf_final_at_k_metrics <- tibble(model = "Random Forest") %>% 
                         bind_cols(final_metrics_at_k(rf_results, 2000))

rf_final_at_k_metrics
```

```{r}
# Gain and Lift Curves
gain_curve(rf_results, response, .pred_yes) %>% 
  autoplot()

lift_curve(rf_results, response, .pred_yes) %>% 
  autoplot()
```

Gain: By approaching 25% of the ordered list , \~ 64% of all interested clients are reached.

Lift: By approaching 25% of the ordered list , the model performed \~2.7 times better than a random list.

# XGBosst ðŸ’»

Time to train the model: 5.701178 mins.

```{r}
# Model Specification -----------
xgb_model <- boost_tree(mtry = 6,
                        trees = 1000,
                        min_n = 28,
                        tree_depth = 14,
                        loss_reduction = 0.01,
                        sample_size = 0.27,
                        ) %>% 
             set_engine('xgboost') %>% 
             set_mode('classification')

# Model Fitting -----------
start_time <- Sys.time()

xgb_fit <- xgb_model %>% 
           fit(response ~., data = df_train_preprocessed)

end_time <- Sys.time()

print(end_time - start_time)

saveRDS(xgb_model, "xgb_model.rds")
```

```{r}
xgb_model <- readRDS("xgb_model.rds")

# Prediction ----------
class_preds <- xgb_fit %>% 
  predict(new_data = df_test_preprocessed,
          type = 'class')

prob_preds <- xgb_fit %>% 
              predict(new_data = df_test_preprocessed, type = 'prob')

# Combine results -----------
xgb_results <- df_test %>% 
               select(id, response) %>% 
               bind_cols(class_preds, prob_preds)

# Confusion Matrix ------------
confusion_matrix_xgb <-  conf_mat(xgb_results, truth = response, estimate = .pred_class)
```

```{r}
# Calculating metrics @K
xgb_final_at_k_metrics <- tibble(model = "XGBoost") %>% 
                          bind_cols(final_metrics_at_k(xgb_results, 2000))

xgb_final_at_k_metrics
```

```{r}
# Gain and Lift Curves
gain_curve(xgb_results, response, .pred_yes) %>% 
  autoplot()

lift_curve(xgb_results, response, .pred_yes) %>% 
  autoplot()
```

Gain: By approaching 25% of the ordered list, \~ 62% of all interested clients are reached.

Lift: By approaching 25% of the ordered list, the model performed \~2.4 times better than a random list.

# KNN

Time to train the model: 19.01394 mins

Time for prediction: 9.316501 mins

```{r}
library(kknn)

# Model Specification -----------
knn_model <- nearest_neighbor(weight_func = "rectangular",
                              neighbors = 3) %>% set_engine('kknn') %>% set_mode('classification')

# Model Fitting -----------
start_time <- Sys.time()

knn_fit <- knn_model %>% 
  fit(response ~., 
      data = df_train_preprocessed)

end_time <- Sys.time()

print(end_time - start_time)

saveRDS(knn_fit, "knn_fit.rds")
```

```{r}
knn_fit <- readRDS("knn_fit.rds")

start_time <- Sys.time()
# Prediction ----------
class_preds <- knn_fit %>% 
  predict(new_data = df_test_preprocessed,
          type = 'class')

prob_preds <- knn_fit %>% 
  predict(new_data = df_test_preprocessed,
          type = 'prob')
end_time <- Sys.time()

print(end_time - start_time)

# Combine results -----------
knn_results <- df_test %>% 
  select(id, response) %>% 
  bind_cols(class_preds, prob_preds)

saveRDS(knn_results, "knn_results.rds") 
```

```{r}
knn_results <- readRDS("knn_results.rds")

# Confusion Matrix ------------
confusion_matrix_knn <-  conf_mat(
  knn_results, truth = response, estimate = .pred_class
)

# Calculating metrics @K
knn_final_at_k_metrics <- tibble(
  model = "KNN"
) %>% 
  bind_cols(final_metrics_at_k(knn_results, 2000))

knn_final_at_k_metrics
```

```{r}
# Gain and Lift Curves
gain_curve(knn_results, response, .pred_yes) %>% 
  autoplot()

lift_curve(knn_results, response, .pred_yes) %>% 
  autoplot()
```

Gain: By approaching 25% of the ordered list, \~ 62% of all interested clients are reached.

Lift: By approaching 25% of the ordered list, the model performed \~2.2 times better than a random list.

```{r}
rf_final_at_k_metrics %>% 
  bind_rows(dt_final_at_k_metrics, 
            lr_final_at_k_metrics,
            xgb_final_at_k_metrics,
            knn_final_at_k_metrics) %>% 
  arrange(desc(recall_at_k))
```
